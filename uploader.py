#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Raceshot åœ–ç‰‡ä¸Šå‚³å·¥å…·

åŠŸèƒ½ï¼š
- ç”±æŒ‡å®šè³‡æ–™å¤¾éè¿´è’é›†åœ–ç‰‡æª”
- é€å¼µå‘¼å« API ä¸Šå‚³ï¼Œæ–¹ä¾¿ç²¾æº–è¨˜éŒ„æ¯å¼µæª”åä¹‹æˆåŠŸ/å¤±æ•—
- å…è¨±å¾å‘½ä»¤åˆ—æˆ–ç’°å¢ƒè®Šæ•¸ RACESHOT_API_TOKEN å–å¾— API Token
- å…·å‚™é‡è©¦èˆ‡é€¾æ™‚è™•ç†ã€éŒ¯èª¤æ—¥èªŒ
- ç”¢å‡ºçµæœæª”æ¡ˆï¼š
  - output/upload_results.csv
  - output/success_list.txt
  - output/failure_list.txt
  - output/upload.log (å®Œæ•´æ—¥èªŒ)

ä½¿ç”¨æ–¹å¼è«‹è¦‹ README.mdã€‚
"""
from __future__ import annotations

import argparse
import csv
import logging
import mimetypes
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import requests
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import hashlib

API_ENDPOINT = "https://api.raceshot.app/api/v1/photographer/upload"
DEFAULT_PRICE = 30

# è¼¸å‡ºç›®éŒ„ï¼šä½¿ç”¨è€…å®¶ç›®éŒ„ï¼ˆé¿å…æ‰“åŒ…å¾Œçš„å”¯è®€å•é¡Œï¼‰
OUTPUT_DIR = Path.home() / ".raceshot_uploader" / "output"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

RESULT_CSV = OUTPUT_DIR / "upload_results.csv"
SUCCESS_LIST = OUTPUT_DIR / "success_list.txt"
FAILURE_LIST = OUTPUT_DIR / "failure_list.txt"
LOG_FILE = OUTPUT_DIR / "upload.log"
HISTORY_CSV = OUTPUT_DIR / "upload_history_v2.csv"
WRITE_LOCK = threading.Lock()

# å…è¨±çš„åœ–ç‰‡å‰¯æª”åï¼ˆå°å¯«ï¼‰
ALLOWED_EXTENSIONS = {".jpg", ".jpeg", ".png"}


@dataclass
class UploadResult:
    file_name: str
    success: bool
    message: str
    photo_id: Optional[str] = None
    error: Optional[str] = None
    status_code: Optional[int] = None
    file_path: Optional[str] = None  # çµ•å°è·¯å¾‘ï¼Œä¾›æ­·å²ç´€éŒ„ä½¿ç”¨
    signature: Optional[str] = None  # æª”æ¡ˆç‰¹å¾µå€¼


def setupLogging() -> None:
    """è¨­å®š console èˆ‡ file çš„ loggingã€‚"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # Console handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    ch.setFormatter(ch_formatter)

    # File handler
    fh = logging.FileHandler(LOG_FILE, encoding="utf-8")
    fh.setLevel(logging.INFO)
    fh_formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    fh.setFormatter(fh_formatter)

    logger.handlers.clear()
    logger.addHandler(ch)
    logger.addHandler(fh)


def parseArgs(argv: Optional[List[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Raceshot åœ–ç‰‡ä¸Šå‚³å·¥å…·")
    parser.add_argument("--dir", dest="directory", required=False, default=None, help="è¦éè¿´ä¸Šå‚³çš„åœ–ç‰‡è³‡æ–™å¤¾ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_DIRï¼‰")
    parser.add_argument("--event-id", dest="event_id", required=False, default=None, help="æ´»å‹• IDï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_EVENT_IDï¼‰")
    parser.add_argument("--location", dest="location", required=False, default=None, help="æ‹æ”åœ°é»ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_LOCATIONï¼‰")
    parser.add_argument("--price", dest="price", type=int, default=None, help=f"åƒ¹æ ¼ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_PRICEï¼›æœªæä¾›å‰‡é è¨­ {DEFAULT_PRICE}ï¼‰")
    parser.add_argument("--bib-number", dest="bib_number", default=None, help="è™Ÿç¢¼å¸ƒè™Ÿç¢¼ (å¯é¸)")
    parser.add_argument("--token", dest="token", default=None, help="API Tokenï¼›å¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_API_TOKEN")
    parser.add_argument("--max-retries", dest="max_retries", type=int, default=None, help="æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_MAX_RETRIESï¼›æœªæä¾›å‰‡é è¨­ 3ï¼‰")
    parser.add_argument("--retry-backoff", dest="retry_backoff", type=float, default=None, help="é‡è©¦é€€é¿ä¿‚æ•¸ç§’æ•¸ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_RETRY_BACKOFFï¼›æœªæä¾›å‰‡é è¨­ 1.5ï¼‰")
    parser.add_argument("--timeout", dest="timeout", type=float, default=None, help="å–®æ¬¡è«‹æ±‚é€¾æ™‚ç§’æ•¸ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_TIMEOUTï¼›æœªæä¾›å‰‡é è¨­ 30sï¼‰")
    parser.add_argument("--dry-run", dest="dry_run", action="store_true", help="åƒ…åˆ—å‡ºå°‡è¦ä¸Šå‚³çš„æª”æ¡ˆï¼Œä¸å¯¦éš›å‘¼å« API")
    parser.add_argument("--env-file", dest="env_file", default=None, help="æŒ‡å®š .env æª”è·¯å¾‘ï¼ˆé è¨­æœƒè‡ªå‹•è®€å–ç•¶å‰ç›®éŒ„çš„ .envï¼‰")
    parser.add_argument("--concurrency", dest="concurrency", type=int, default=None, help="åŒæ™‚ä¸Šå‚³çš„å·¥ä½œåŸ·è¡Œç·’æ•¸ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_CONCURRENCYï¼›é è¨­ 1ï¼‰")
    parser.add_argument("--batch-size", dest="batch_size", type=int, default=None, help="å–®æ¬¡è«‹æ±‚ä¸Šå‚³çš„åœ–ç‰‡æ•¸é‡ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_BATCH_SIZEï¼›é è¨­ 1ï¼‰")
    parser.add_argument("--reupload-failures", dest="reupload_failures", action="store_true", help="è®€å– output/failure_list.txt ä¸¦åƒ…é‡æ–°ä¸Šå‚³é€™äº›å¤±æ•—çš„æª”æ¡ˆ")
    parser.add_argument("--longitude", dest="longitude", type=float, default=None, help="ç¶“åº¦ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_LONGITUDEï¼‰")
    parser.add_argument("--latitude", dest="latitude", type=float, default=None, help="ç·¯åº¦ï¼ˆå¯ç”¨ç’°å¢ƒè®Šæ•¸ RACESHOT_LATITUDEï¼‰")
    return parser.parse_args(argv)


def getApiToken(cli_token: Optional[str]) -> str:
    token = cli_token or os.getenv("RACESHOT_API_TOKEN")
    if not token:
        logging.error("æ‰¾ä¸åˆ° API Tokenï¼Œè«‹ä½¿ç”¨ --token æˆ–è¨­å®šç’°å¢ƒè®Šæ•¸ RACESHOT_API_TOKEN")
        sys.exit(1)
    return token


def parseBoolEnv(val: Optional[str]) -> Optional[bool]:
    """å°‡ç’°å¢ƒè®Šæ•¸å­—ä¸²è½‰ç‚ºå¸ƒæ—ã€‚
    å¯æ¥å—ï¼š1/true/yes/y/on èˆ‡ 0/false/no/n/offï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰ã€‚
    ç„¡æ³•è§£æå‰‡å›å‚³ Noneã€‚
    """
    if val is None:
        return None
    s = val.strip().lower()
    if s in {"1", "true", "yes", "y", "on"}:
        return True
    if s in {"0", "false", "no", "n", "off"}:
        return False
    return None


def collectImageFiles(root_dir: Path) -> List[Path]:
    if not root_dir.exists() or not root_dir.is_dir():
        logging.error(f"æŒ‡å®šè·¯å¾‘ä¸å­˜åœ¨æˆ–ä¸æ˜¯è³‡æ–™å¤¾ï¼š{root_dir}")
        sys.exit(1)

    files: List[Path] = []
    for path in root_dir.rglob("*"):
        if path.is_file():
            ext = path.suffix.lower()
            if ext in ALLOWED_EXTENSIONS:
                files.append(path)
    files.sort()
    if not files:
        logging.warning("æ‰¾ä¸åˆ°ä»»ä½•ç¬¦åˆçš„åœ–ç‰‡æª”æ¡ˆã€‚å…è¨±çš„å‰¯æª”åï¼š" + ", ".join(sorted(ALLOWED_EXTENSIONS)))
    logging.info(f"å…±æ‰¾åˆ° {len(files)} å¼µåœ–ç‰‡å¾…ä¸Šå‚³")
    return files


def guessMimeType(file_path: Path) -> Optional[str]:
    mime, _ = mimetypes.guess_type(str(file_path))
    return mime


def getFileSignature(path: Path) -> str:
    """è¨ˆç®—æª”æ¡ˆç‰¹å¾µå€¼ï¼šHash(Size + Mtime + First 4KB)"""
    try:
        stat = path.stat()
        file_size = stat.st_size
        mtime = stat.st_mtime
        
        with open(path, "rb") as f:
            head = f.read(4096)
        
        h = hashlib.md5()
        h.update(str(file_size).encode())
        h.update(str(mtime).encode())
        h.update(head)
        return h.hexdigest()
    except Exception as e:
        logging.warning(f"ç„¡æ³•è¨ˆç®—æª”æ¡ˆç‰¹å¾µå€¼ {path}: {e}")
        return ""


def buildMultipart(file_path: Path) -> Tuple[str, Tuple[str, bytes, Optional[str]]]:
    """å»ºç«‹ multipart form-data çš„å–®ä¸€ 'images' æ¬„ä½è³‡æ–™ã€‚
    å›å‚³ (field_name, file_tuple)
    file_tuple ç‚º (filename, file_bytes, content_type)
    """
    content_type = guessMimeType(file_path) or "application/octet-stream"
    with open(file_path, "rb") as f:
        data = f.read()
    return (
        "images",
        (file_path.name, data, content_type),
    )


def chunked(items: List[Path], size: int) -> List[List[Path]]:
    """å°‡æ¸…å–®åˆ‡æˆå›ºå®šå¤§å°å€å¡Šã€‚"""
    if size <= 0:
        size = 1
    return [items[i : i + size] for i in range(0, len(items), size)]


def shouldRetry(status_code: Optional[int], exc: Optional[BaseException]) -> bool:
    if exc is not None:
        # é€£ç·šéŒ¯èª¤ã€é€¾æ™‚ç­‰ï¼Œå»ºè­°é‡è©¦
        return True
    if status_code is None:
        return False
    # 5xx èˆ‡ 429 é‡è©¦
    if status_code >= 500 or status_code == 429:
        return True
    return False


def isDuplicateFailure(error_msg: Optional[str], failure_item: Optional[dict] = None) -> Tuple[bool, Optional[str]]:
    """åˆ¤æ–·æ˜¯å¦ç‚ºã€å·²ä¸Šå‚³ã€çš„é‡è¤‡æƒ…æ³ï¼Œä¸¦å˜—è©¦å–å‡º photoIdã€‚
    è¦å‰‡ï¼š
    - error å«æœ‰ 'already upload'ï¼ˆä¸åˆ†å¤§å°å¯«ï¼‰ã€'å·²ä¸Šå‚³'ã€'å·²å­˜åœ¨' ç­‰å­—æ¨£
    - æˆ– failure_item æœ¬èº«å«æœ‰ photoIdï¼ˆå¤šæ•¸æƒ…æ³ä»£è¡¨å·²å­˜åœ¨ï¼‰
    å›å‚³ (is_duplicate, photo_id)
    """
    photo_id = None
    if isinstance(failure_item, dict):
        pid = failure_item.get("photoId") or failure_item.get("photoID")
        if pid:
            photo_id = str(pid)
    if error_msg:
        s = str(error_msg).strip().lower()
        dup_keywords = ["already upload", "already uploaded", "duplicate", "å·²ä¸Šå‚³", "å·²å­˜åœ¨"]
        if any(k in s for k in dup_keywords):
            return True, photo_id
    # æ²’æœ‰æ˜ç¢ºè¨Šæ¯ï¼Œä½†æœ‰ photo_id ä¹Ÿè¦–ç‚ºå·²å­˜åœ¨
    if photo_id:
        return True, photo_id
    return False, None


def uploadSingleImage(
    session: requests.Session,
    token: str,
    file_path: Path,
    event_id: str,
    location: str,
    price: int,
    bib_number: Optional[str],
    timeout: float,
    max_retries: int,
    retry_backoff: float,
    longitude: Optional[float] = None,
    latitude: Optional[float] = None,
) -> UploadResult:
    headers = {"Authorization": f"Bearer {token}"}

    form_data = {
        "eventId": str(event_id),
        "location": str(location),
        "price": str(price),
    }
    if bib_number:
        form_data["bibNumber"] = str(bib_number)
    if longitude is not None:
        form_data["longitude"] = str(longitude)
    if latitude is not None:
        form_data["latitude"] = str(latitude)

    images_field = [buildMultipart(file_path)]

    attempt = 0
    last_error: Optional[str] = None
    last_status: Optional[int] = None
    while True:
        attempt += 1
        exc: Optional[BaseException] = None
        try:
            resp = session.post(
                API_ENDPOINT,
                headers=headers,
                data=form_data,
                files=images_field,
                timeout=timeout,
            )
            last_status = resp.status_code
            # å˜—è©¦è§£æ JSON
            payload = None
            try:
                payload = resp.json()
            except Exception:
                payload = None

            if resp.ok and payload is not None:
                # ä¾ç…§é¡Œç›®å›å‚³æ ¼å¼è™•ç†
                success = bool(payload.get("success"))
                message = str(payload.get("message", ""))
                photo_ids = payload.get("photoIds") or []
                failure_items = payload.get("failures") or []

                if success:
                    photo_id = photo_ids[0] if isinstance(photo_ids, list) and photo_ids else None
                    logging.info(f"âœ… æˆåŠŸä¸Šå‚³ï¼š{file_path.name} (photoId={photo_id})")
                    return UploadResult(
                        file_name=file_path.name,
                        success=True,
                        message=message or "ä¸Šå‚³æˆåŠŸ",
                        photo_id=photo_id,
                        status_code=resp.status_code,
                        file_path=str(file_path.resolve()),
                        signature=getFileSignature(file_path),
                    )
                else:
                    # å¤±æ•—æƒ…å¢ƒï¼šå›å‚³ä¸­å¯èƒ½å«æœ‰ failures é™£åˆ—
                    err_msg = None
                    if isinstance(failure_items, list) and failure_items:
                        f0 = failure_items[0]
                        # å¾Œç«¯å¯èƒ½å‚³ error èˆ‡ fileName æˆ– photoId
                        err_msg = f0.get("error") or message or "ä¸Šå‚³å¤±æ•—"
                        is_dup, dup_pid = isDuplicateFailure(err_msg, f0)
                        if is_dup:
                            logging.info(f"â˜‘ï¸ å·²ä¸Šå‚³ï¼ˆè¦–ç‚ºæˆåŠŸï¼‰ï¼š{file_path.name}")
                            return UploadResult(
                                file_name=file_path.name,
                                success=True,
                                message="å·²ä¸Šå‚³ï¼ˆè¦–ç‚ºæˆåŠŸï¼‰",
                                photo_id=dup_pid,
                                status_code=resp.status_code,
                                file_path=str(file_path.resolve()),
                                signature=getFileSignature(file_path),
                            )
                    else:
                        err_msg = message or f"HTTP {resp.status_code}"
                    logging.warning(f"âš ï¸ ä¸Šå‚³å¤±æ•—ï¼š{file_path.name} - {err_msg}")
                    # 4xx ä¸é‡è©¦ï¼›ä½† 429 æœƒåœ¨ shouldRetry ä¸­å…è¨±
                    if not shouldRetry(resp.status_code, None):
                        return UploadResult(
                            file_name=file_path.name,
                            success=False,
                            message=message or "ä¸Šå‚³å¤±æ•—",
                            error=err_msg,
                            status_code=resp.status_code,
                            file_path=str(file_path.resolve()),
                            signature=getFileSignature(file_path),
                        )
                    last_error = err_msg
            else:
                # é ok æˆ–é JSON å›å‚³
                text = None
                try:
                    text = resp.text
                except Exception:
                    text = None
                last_error = f"HTTP {resp.status_code}: {text[:300] if text else 'No body'}"
                logging.warning(f"âš ï¸ ä¼ºæœå™¨å›æ‡‰éŒ¯èª¤ï¼š{file_path.name} - {last_error}")
        except (requests.ConnectionError, requests.Timeout) as e:
            exc = e
            last_error = f"é€£ç·š/é€¾æ™‚éŒ¯èª¤ï¼š{e}"
            logging.warning(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼ˆ{file_path.name}ï¼‰ï¼š{e}")
        except Exception as e:
            exc = e
            last_error = f"å…¶ä»–éŒ¯èª¤ï¼š{e}"
            logging.exception(f"âŒ æœªé æœŸéŒ¯èª¤ï¼ˆ{file_path.name}ï¼‰ï¼š{e}")

        # åˆ¤æ–·æ˜¯å¦é‡è©¦
        if attempt <= max_retries and shouldRetry(last_status, exc):
            sleep_seconds = retry_backoff ** (attempt - 1)
            logging.info(f"é‡è©¦ç¬¬ {attempt}/{max_retries} æ¬¡å‰ç­‰å¾… {sleep_seconds:.1f}sï¼š{file_path.name}")
            time.sleep(sleep_seconds)
            continue

        # æ”¾æ£„é‡è©¦
        return UploadResult(
            file_name=file_path.name,
            success=False,
            message="ä¸Šå‚³å¤±æ•—",
            error=last_error,
            status_code=last_status,
            file_path=str(file_path.resolve()),
            signature=getFileSignature(file_path),
        )


def uploadImagesBatch(
    token: str,
    file_paths: List[Path],
    event_id: str,
    location: str,
    price: int,
    bib_number: Optional[str],
    timeout: float,
    max_retries: int,
    retry_backoff: float,
    longitude: Optional[float] = None,
    latitude: Optional[float] = None,
) -> List[UploadResult]:
    """ä¸€æ¬¡ä¸Šå‚³å¤šå¼µåœ–ç‰‡ï¼ˆåŒä¸€è«‹æ±‚ï¼‰ï¼Œå›å‚³é€æª”çµæœæ¸…å–®ã€‚"""
    headers = {"Authorization": f"Bearer {token}"}
    form_data = {
        "eventId": str(event_id),
        "location": str(location),
        "price": str(price),
    }
    if bib_number:
        form_data["bibNumber"] = str(bib_number)
    if longitude is not None:
        form_data["longitude"] = str(longitude)
    if latitude is not None:
        form_data["latitude"] = str(latitude)

    files_field: List[Tuple[str, Tuple[str, bytes, Optional[str]]]] = []
    for p in file_paths:
        files_field.append(buildMultipart(p))

    attempt = 0
    last_error: Optional[str] = None
    last_status: Optional[int] = None
    session = requests.Session()
    while True:
        attempt += 1
        exc: Optional[BaseException] = None
        try:
            resp = session.post(
                API_ENDPOINT,
                headers=headers,
                data=form_data,
                files=files_field,
                timeout=timeout,
            )
            last_status = resp.status_code
            payload = None
            try:
                payload = resp.json()
            except Exception:
                payload = None

            # é è¨­å…¨éƒ¨æ¨™ç‚ºå¤±æ•—ï¼Œè‹¥æˆåŠŸæˆ–åˆ¤å®šç‚ºé‡è¤‡å‰‡è¦†å¯«
            results: List[UploadResult] = [
                UploadResult(
                    file_name=p.name,
                    success=False,
                    message="ä¸Šå‚³å¤±æ•—",
                    file_path=str(p.resolve()),
                    signature=getFileSignature(p)
                )
                for p in file_paths
            ]

            if resp.ok and payload is not None:
                success = bool(payload.get("success"))
                message = str(payload.get("message", ""))
                photo_ids = payload.get("photoIds") or []
                failure_items = payload.get("failures") or []

                # å°‡å¤±æ•—é …ç›®ä»¥ fileName å»ºç«‹æŸ¥è¡¨
                failed_by_name: dict[str, str] = {}
                dup_by_name: dict[str, Optional[str]] = {}
                unknown_failure_count = 0
                for f in failure_items if isinstance(failure_items, list) else []:
                    fn = f.get("fileName") or f.get("filename")
                    err = f.get("error") or message or "ä¸Šå‚³å¤±æ•—"
                    is_dup, dup_pid = isDuplicateFailure(err, f)
                    if fn:
                        if is_dup:
                            dup_by_name[fn] = dup_pid
                        else:
                            failed_by_name[fn] = err
                    else:
                        # ç„¡æ³•å°æ‡‰åˆ°æª”åè€…
                        if not is_dup:
                            unknown_failure_count += 1

                # å…ˆæ¨™è¨˜ã€å·²ä¸Šå‚³ã€ç‚ºæˆåŠŸ
                for r in results:
                    if r.file_name in dup_by_name:
                        r.success = True
                        r.message = "å·²ä¸Šå‚³ï¼ˆè¦–ç‚ºæˆåŠŸï¼‰"
                        r.photo_id = dup_by_name[r.file_name]

                # å†æ¨™è¨˜çœŸæ­£çš„å¤±æ•—
                for r in results:
                    if r.file_name in failed_by_name:
                        r.success = False
                        r.error = failed_by_name[r.file_name]
                        r.message = message or r.error or "ä¸Šå‚³å¤±æ•—"

                # å…¶é¤˜æ¨™è¨˜ç‚ºæˆåŠŸï¼ˆæš«ç„¡æ³•å¾å›å‚³å°æ‡‰ file_name èˆ‡ photo_id çš„é—œä¿‚ï¼‰
                for i, r in enumerate(results):
                    if r.file_name not in failed_by_name and r.file_name not in dup_by_name:
                        r.success = True
                        r.message = message or "ä¸Šå‚³æˆåŠŸ"
                        r.photo_id = None

                # è‹¥å›å‚³æœ‰æœªçŸ¥çš„å¤±æ•—æ•¸ï¼ˆæ²’æœ‰ fileNameï¼‰ï¼Œå°‡å°šæœªæ¨™è¨˜çš„æˆåŠŸé …ç›®å›å¡«ç‚ºå¤±æ•—ä»¥ç¬¦åˆè¨ˆæ•¸
                if unknown_failure_count > 0:
                    patched = 0
                    for r in results:
                        if patched >= unknown_failure_count:
                            break
                        if r.success and r.file_name not in dup_by_name:
                            r.success = False
                            r.error = "å›æ‡‰æœªæä¾› fileNameï¼Œç„¡æ³•å°æ‡‰ä¹‹å¤±æ•—é …ç›®"
                            r.message = message or r.error
                            patched += 1

                # å˜—è©¦å¡«å…¥ç¬¬ä¸€å€‹ photo_id æ–¼ç¬¬ä¸€å€‹æˆåŠŸé …ï¼ˆåƒ…ä½œç‚ºåƒè€ƒï¼‰
                if isinstance(photo_ids, list) and photo_ids:
                    for r in results:
                        if r.success:
                            r.photo_id = photo_ids[0]
                            break

                # è¨˜éŒ„æ—¥èªŒ
                ok_count = sum(1 for r in results if r.success)
                fail_count = len(results) - ok_count
                logging.info(
                    f"ğŸ“¦ æ‰¹æ¬¡ä¸Šå‚³å®Œæˆï¼šæˆåŠŸ {ok_count}ã€å¤±æ•— {fail_count}ï¼ˆæ‰¹æ¬¡å¤§å° {len(file_paths)}ï¼‰"
                )
                return results
            else:
                text = None
                try:
                    text = resp.text
                except Exception:
                    text = None
                last_error = f"HTTP {resp.status_code}: {text[:300] if text else 'No body'}"
                logging.warning(f"âš ï¸ ä¼ºæœå™¨å›æ‡‰éŒ¯èª¤ï¼ˆæ‰¹æ¬¡ï¼‰ï¼š{last_error}")
        except (requests.ConnectionError, requests.Timeout) as e:
            exc = e
            last_error = f"é€£ç·š/é€¾æ™‚éŒ¯èª¤ï¼š{e}"
            logging.warning(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼ˆæ‰¹æ¬¡ï¼‰ï¼š{e}")
        except Exception as e:
            exc = e
            last_error = f"å…¶ä»–éŒ¯èª¤ï¼š{e}"
            logging.exception(f"âŒ æœªé æœŸéŒ¯èª¤ï¼ˆæ‰¹æ¬¡ï¼‰ï¼š{e}")

        if attempt <= max_retries and shouldRetry(last_status, exc):
            sleep_seconds = retry_backoff ** (attempt - 1)
            logging.info(f"æ‰¹æ¬¡é‡è©¦ç¬¬ {attempt}/{max_retries} æ¬¡å‰ç­‰å¾… {sleep_seconds:.1f}s")
            time.sleep(sleep_seconds)
            continue

        # æ”¾æ£„é‡è©¦ï¼Œå…¨éƒ¨æ¨™å¤±æ•—ä¸¦å›å‚³
        results: List[UploadResult] = [
            UploadResult(
                file_name=p.name,
                success=False,
                message="ä¸Šå‚³å¤±æ•—",
                error=last_error,
                status_code=last_status,
                file_path=str(p.resolve()),
                signature=getFileSignature(p),
            )
            for p in file_paths
        ]
        return results

def init_results_files() -> None:
    """åˆå§‹åŒ–çµæœè¼¸å‡ºæª”æ¡ˆï¼šè‹¥ä¸å­˜åœ¨ï¼Œå»ºç«‹ä¸¦å¯«å…¥å¿…è¦çš„è¡¨é ­ã€‚"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    if not RESULT_CSV.exists():
        with open(RESULT_CSV, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["file_path", "file_name", "success", "message", "photo_id", "error", "status_code"])
        logging.info(f"å»ºç«‹çµæœ CSVï¼š{RESULT_CSV}")

    if not SUCCESS_LIST.exists():
        SUCCESS_LIST.touch()
        logging.info(f"å»ºç«‹æˆåŠŸæ¸…å–®ï¼š{SUCCESS_LIST}")

    if not FAILURE_LIST.exists():
        FAILURE_LIST.touch()
        logging.info(f"å»ºç«‹å¤±æ•—æ¸…å–®ï¼š{FAILURE_LIST}")

    if not HISTORY_CSV.exists():
        with open(HISTORY_CSV, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["signature", "event_id", "photo_id", "file_path", "uploaded_at"])
        logging.info(f"å»ºç«‹æ­·å²ç´€éŒ„ CSVï¼š{HISTORY_CSV}")


def append_results(results: List[UploadResult], event_id: str, location: str) -> None:
    """å°‡ä¸€æ‰¹çµæœè¿½åŠ åˆ°è¼¸å‡ºæª”æ¡ˆï¼ˆCSVã€æˆåŠŸ/å¤±æ•—æ¸…å–®ã€æ­·å²ç´€éŒ„ï¼‰ã€‚"""
    with WRITE_LOCK:
        # CSV é€åˆ—è¿½åŠ 
        with open(RESULT_CSV, "a", newline="", encoding="utf-8") as fcsv:
            writer = csv.writer(fcsv)
            for r in results:
                writer.writerow([
                    r.file_path or "",
                    r.file_name,
                    r.success,
                    r.message,
                    r.photo_id or "",
                    r.error or "",
                    r.status_code or "",
                ])

        # æˆåŠŸ/å¤±æ•—é€åˆ—è¿½åŠ ï¼ˆä¿®æ”¹ï¼šè¨˜éŒ„çµ•å°è·¯å¾‘ä»¥ä¾¿ç²¾æº–é‡è©¦ï¼‰
        if results:
            with open(SUCCESS_LIST, "a", encoding="utf-8") as fsucc, open(FAILURE_LIST, "a", encoding="utf-8") as ffail:
                for r in results:
                    path_to_write = r.file_path or r.file_name
                    if r.success:
                        fsucc.write(path_to_write + "\n")
                    else:
                        ffail.write(path_to_write + "\n")

        # æ­·å²ç´€éŒ„ï¼šåƒ…é‡å°æˆåŠŸ
        with open(HISTORY_CSV, "a", newline="", encoding="utf-8") as fh:
            writer = csv.writer(fh)
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            for r in results:
                if r.success and r.signature:
                    writer.writerow([
                        r.signature,
                        event_id,
                        r.photo_id or "",
                        r.file_path or "",
                        timestamp
                    ])


def collect_failures_to_reupload(root_dir: Path) -> List[Path]:
    """è®€å– failure_list.txt ä¸¦åˆ¤æ–·æ˜¯è·¯å¾‘é‚„æ˜¯æª”åï¼Œå›å‚³å¾…ä¸Šå‚³æ¸…å–®ã€‚"""
    if not FAILURE_LIST.exists():
        logging.error(f"æ‰¾ä¸åˆ°å¤±æ•—æ¸…å–®æª”æ¡ˆï¼š{FAILURE_LIST}")
        sys.exit(1)

    with open(FAILURE_LIST, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    if not lines:
        logging.info("å¤±æ•—æ¸…å–®æ˜¯ç©ºçš„ï¼Œæ²’æœ‰éœ€è¦é‡æ–°ä¸Šå‚³çš„æª”æ¡ˆã€‚")
        return []

    files_to_reupload: List[Path] = []
    
    # ç­–ç•¥ï¼š
    # 1. å¦‚æœé€™ä¸€è¡Œçœ‹èµ·ä¾†åƒçµ•å°è·¯å¾‘ä¸”æª”æ¡ˆå­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨
    # 2. å¦‚æœåªæ˜¯æª”åï¼Œå‰‡æƒæ root_dir å°‹æ‰¾å°æ‡‰ï¼ˆå¯èƒ½æœƒæœ‰å¤šå€‹åŒåæª”æ¡ˆï¼Œå»ºè­°å…¨éƒ¨åŠ å…¥å˜—è©¦ä¸Šå‚³ï¼Œæˆ–æ˜¯åƒ…ç¬¬ä¸€å€‹ï¼‰
    #    ç”±æ–¼èˆŠç‰ˆé‚è¼¯æœ‰ç¼ºé™·ï¼ˆåŒåè¦†è“‹ï¼‰ï¼Œé€™è£¡æ”¹ç‚ºï¼šè‹¥ç‚ºæª”åï¼Œå‰‡æ”¶é›†æ‰€æœ‰åŒåæª”æ¡ˆ

    # å…ˆæƒæ root_dir å»ºç«‹æª”åç´¢å¼•ï¼ˆåƒ…åœ¨æœ‰éœ€è¦æ™‚æ‰åšï¼Œå„ªåŒ–æ•ˆèƒ½ï¼‰
    name_map: Optional[dict[str, List[Path]]] = None

    def get_name_map() -> dict[str, List[Path]]:
        logging.info(f"æ­£åœ¨æƒæ {root_dir} ä»¥åŒ¹é…å¤±æ•—çš„æª”åâ€¦")
        m = {}
        for p in collectImageFiles(root_dir):
            m.setdefault(p.name, []).append(p)
        return m

    count_by_path = 0
    count_by_name = 0

    for line in lines:
        p = Path(line)
        if p.is_absolute() and p.exists():
            files_to_reupload.append(p)
            count_by_path += 1
        else:
            # è¦–ç‚ºæª”åï¼ˆèˆŠæ ¼å¼æˆ–ç›¸å°è·¯å¾‘ï¼‰
            if name_map is None:
                name_map = get_name_map()
            
            # ä½¿ç”¨æª”ååŒ¹é…
            fname = p.name
            if fname in name_map:
                # å°‡æ‰€æœ‰åŒåæª”æ¡ˆéƒ½åŠ å…¥é‡è©¦ï¼Œç”±å¾ŒçºŒçš„ Signature æ©Ÿåˆ¶å»éæ¿¾é‡è¤‡
                for match_p in name_map[fname]:
                    files_to_reupload.append(match_p)
                count_by_name += 1
            else:
                logging.warning(f"æ‰¾ä¸åˆ°æª”æ¡ˆï¼ˆæ—¢éçµ•å°è·¯å¾‘ï¼Œä¹Ÿç„¡åŒåæª”æ¡ˆï¼‰ï¼š{line}")

    # å»é‡
    unique_files = list(set(files_to_reupload))
    logging.info(f"å¾å¤±æ•—æ¸…å–®è§£æå‡º {len(unique_files)} å€‹å¾…é‡è©¦æª”æ¡ˆï¼ˆè·¯å¾‘åŒ¹é…: {count_by_path}, æª”ååŒ¹é…: {count_by_name}ï¼‰")
    return unique_files


def read_history_keys(event_id: str) -> set:
    """è®€å–æ­·å²ç´€éŒ„ï¼Œå›å‚³å·²æˆåŠŸä¸Šå‚³éçš„ (signature, event_id) é›†åˆã€‚"""
    keys = set()
    if not HISTORY_CSV.exists():
        return keys
    try:
        with open(HISTORY_CSV, "r", newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader, None)
            # csv header: signature, event_id, photo_id, file_path, uploaded_at
            for row in reader:
                if len(row) >= 2:
                    sig = row[0]
                    evt = row[1]
                    if evt == str(event_id):
                        keys.add((sig, evt))
    except Exception as e:
        logging.warning(f"è®€å–æ­·å²ç´€éŒ„å¤±æ•—ï¼š{e}")
    return keys


def main(argv: Optional[List[str]] = None) -> None:
    args = parseArgs(argv)
    setupLogging()

    # è®€å– .envï¼ˆè‹¥æŒ‡å®šè·¯å¾‘å‰‡ä½¿ç”¨è©²æª”æ¡ˆï¼Œå¦å‰‡å˜—è©¦è¼‰å…¥é è¨­ .envï¼‰
    if args.env_file:
        dotenv_path = Path(args.env_file).expanduser().resolve()
        if dotenv_path.exists():
            loaded = load_dotenv(dotenv_path=dotenv_path, override=False)
            if loaded:
                logging.info(f"å·²è¼‰å…¥ç’°å¢ƒæª”ï¼š{dotenv_path}")
            else:
                logging.warning(f"æœªèƒ½è¼‰å…¥ç’°å¢ƒæª”ï¼š{dotenv_path}")
        else:
            logging.warning(f"æŒ‡å®šçš„ .env æª”æ¡ˆä¸å­˜åœ¨ï¼š{dotenv_path}")
    else:
        loaded = load_dotenv()
        if loaded:
            logging.info("å·²è¼‰å…¥ .env ç’°å¢ƒè®Šæ•¸ï¼ˆé è¨­è·¯å¾‘ï¼‰")

    # ç”±å‘½ä»¤åˆ—åƒæ•¸èˆ‡ç’°å¢ƒè®Šæ•¸åˆä½µæœ€çµ‚è¨­å®šï¼ˆCLI > ENVï¼‰
    env_dir = os.getenv("RACESHOT_DIR")
    env_event_id = os.getenv("RACESHOT_EVENT_ID")
    env_location = os.getenv("RACESHOT_LOCATION")
    env_price = os.getenv("RACESHOT_PRICE")
    env_bib = os.getenv("RACESHOT_BIB_NUMBER")
    env_max_retries = os.getenv("RACESHOT_MAX_RETRIES")
    env_retry_backoff = os.getenv("RACESHOT_RETRY_BACKOFF")
    env_timeout = os.getenv("RACESHOT_TIMEOUT")
    env_dry_run = os.getenv("RACESHOT_DRY_RUN")
    env_concurrency = os.getenv("RACESHOT_CONCURRENCY")
    env_batch_size = os.getenv("RACESHOT_BATCH_SIZE")
    env_longitude = os.getenv("RACESHOT_LONGITUDE")
    env_latitude = os.getenv("RACESHOT_LATITUDE")

    directory = args.directory or env_dir
    event_id = args.event_id or env_event_id
    location = args.location or env_location

    # æ•¸å€¼å‹åƒæ•¸è§£æï¼ˆå¸¶æœ‰é˜²å‘†èˆ‡é è¨­å€¼ï¼‰
    price_eff: int
    if args.price is not None:
        price_eff = int(args.price)
    else:
        try:
            price_eff = int(env_price) if env_price is not None else DEFAULT_PRICE
        except Exception:
            logging.warning(f"RACESHOT_PRICE ç„¡æ³•è§£æç‚ºæ•´æ•¸ï¼š{env_price!r}ï¼Œæ”¹ç”¨é è¨­ {DEFAULT_PRICE}")
            price_eff = DEFAULT_PRICE

    max_retries_eff: int
    if args.max_retries is not None:
        max_retries_eff = int(args.max_retries)
    else:
        try:
            max_retries_eff = int(env_max_retries) if env_max_retries is not None else 3
        except Exception:
            logging.warning(f"RACESHOT_MAX_RETRIES ç„¡æ³•è§£æç‚ºæ•´æ•¸ï¼š{env_max_retries!r}ï¼Œæ”¹ç”¨é è¨­ 3")
            max_retries_eff = 3

    retry_backoff_eff: float
    if args.retry_backoff is not None:
        retry_backoff_eff = float(args.retry_backoff)
    else:
        try:
            retry_backoff_eff = float(env_retry_backoff) if env_retry_backoff is not None else 1.5
        except Exception:
            logging.warning(f"RACESHOT_RETRY_BACKOFF ç„¡æ³•è§£æç‚ºæµ®é»æ•¸ï¼š{env_retry_backoff!r}ï¼Œæ”¹ç”¨é è¨­ 1.5")
            retry_backoff_eff = 1.5

    timeout_eff: float
    if args.timeout is not None:
        timeout_eff = float(args.timeout)
    else:
        try:
            timeout_eff = float(env_timeout) if env_timeout is not None else 30.0
        except Exception:
            logging.warning(f"RACESHOT_TIMEOUT ç„¡æ³•è§£æç‚ºæµ®é»æ•¸ï¼š{env_timeout!r}ï¼Œæ”¹ç”¨é è¨­ 30.0")
            timeout_eff = 30.0

    dry_run_eff = args.dry_run if args.dry_run else (parseBoolEnv(env_dry_run) or False)
    bib_number = args.bib_number or env_bib
    
    # è§£æç¶“ç·¯åº¦
    longitude_eff: Optional[float] = None
    if args.longitude is not None:
        longitude_eff = float(args.longitude)
    elif env_longitude is not None:
        try:
            longitude_eff = float(env_longitude)
        except Exception:
            logging.warning(f"RACESHOT_LONGITUDE ç„¡æ³•è§£æç‚ºæµ®é»æ•¸ï¼š{env_longitude!r}")
    
    latitude_eff: Optional[float] = None
    if args.latitude is not None:
        latitude_eff = float(args.latitude)
    elif env_latitude is not None:
        try:
            latitude_eff = float(env_latitude)
        except Exception:
            logging.warning(f"RACESHOT_LATITUDE ç„¡æ³•è§£æç‚ºæµ®é»æ•¸ï¼š{env_latitude!r}")
    # è§£æä½µç™¼èˆ‡æ‰¹æ¬¡
    if args.concurrency is not None:
        concurrency_eff = max(1, int(args.concurrency))
    else:
        try:
            concurrency_eff = max(1, int(env_concurrency)) if env_concurrency is not None else 1
        except Exception:
            logging.warning(f"RACESHOT_CONCURRENCY ç„¡æ³•è§£æç‚ºæ•´æ•¸ï¼š{env_concurrency!r}ï¼Œæ”¹ç”¨ 1")
            concurrency_eff = 1

    if args.batch_size is not None:
        batch_size_eff = max(1, int(args.batch_size))
    else:
        try:
            batch_size_eff = max(1, int(env_batch_size)) if env_batch_size is not None else 1
        except Exception:
            logging.warning(f"RACESHOT_BATCH_SIZE ç„¡æ³•è§£æç‚ºæ•´æ•¸ï¼š{env_batch_size!r}ï¼Œæ”¹ç”¨ 1")
            batch_size_eff = 1

    # æª¢æŸ¥å¿…å¡«ï¼ˆå…è¨±å¾ .env æˆ– CLI ä»»ä¸€æä¾›ï¼‰
    missing: List[str] = []
    if not directory:
        missing.append("RACESHOT_DIR æˆ– --dir")
    if not event_id:
        missing.append("RACESHOT_EVENT_ID æˆ– --event-id")
    if not location:
        missing.append("RACESHOT_LOCATION æˆ– --location")
    if missing:
        logging.error("ç¼ºå°‘å¿…å¡«åƒæ•¸ï¼š" + ", ".join(missing))
        sys.exit(1)

    token = getApiToken(args.token)
    root_dir = Path(directory).expanduser().resolve()

    files: List[Path]
    if args.reupload_failures:
        logging.info(f"--reupload-failures å•Ÿç”¨ï¼Œå°‡å¾ {FAILURE_LIST} é‡æ–°ä¸Šå‚³å¤±æ•—æª”æ¡ˆ")
        files = collect_failures_to_reupload(root_dir)
        if not files:
            logging.info("å¤±æ•—æ¸…å–®ä¸­æ²’æœ‰éœ€è¦é‡æ–°ä¸Šå‚³çš„æª”æ¡ˆã€‚")
            return
        # æ¸…ç©ºèˆŠçš„å¤±æ•—æ¸…å–®ï¼Œä»¥ä¾¿è¨˜éŒ„æœ¬æ¬¡åŸ·è¡Œçš„å¤±æ•—
        with WRITE_LOCK:
            if FAILURE_LIST.exists():
                logging.info(f"æ¸…ç©ºèˆŠçš„å¤±æ•—æ¸…å–®ï¼š{FAILURE_LIST}")
                FAILURE_LIST.unlink()
            FAILURE_LIST.touch()
    else:
        files = collectImageFiles(root_dir)

    # è®€å–æ­·å²ç´€éŒ„ä¸¦éæ¿¾é‡è¤‡ï¼ˆåŒä¸€ event_id ä¸‹åŒä¸€æª”æ¡ˆçµ•å°è·¯å¾‘è¦–ç‚ºå·²ä¸Šå‚³ï¼‰
    # è®€å–æ­·å²ç´€éŒ„ä¸¦éæ¿¾é‡è¤‡ï¼ˆæ”¹ç”¨ Signature æª¢æŸ¥ï¼‰
    init_results_files()
    history_keys = read_history_keys(event_id)
    
    # è¨ˆç®—ä¸¦éæ¿¾
    final_files: List[Path] = []
    skipped_count = 0
    
    if not history_keys:
        final_files = files
    else:
        logging.info("æ­£åœ¨æª¢æŸ¥æª”æ¡ˆç‰¹å¾µå€¼ä»¥éæ¿¾é‡è¤‡â€¦")
        for p in files:
            sig = getFileSignature(p)
            if (sig, str(event_id)) in history_keys:
                skipped_count += 1
            else:
                final_files.append(p)
    
    if skipped_count > 0:
        logging.info(f"è·³é {skipped_count} å¼µå…·ç›¸åŒç‰¹å¾µå€¼çš„å·²ä¸Šå‚³æª”æ¡ˆï¼ˆevent_id={event_id}ï¼‰")
    
    files = final_files

    if dry_run_eff:
        for p in files:
            logging.info(f"DRY-RUN å°‡ä¸Šå‚³ï¼š{p} (Sig={getFileSignature(p)})")
        logging.info("Dry run çµæŸï¼Œæœªå‘¼å« APIã€‚")
        return

    results: List[UploadResult] = []
    if concurrency_eff == 1 and batch_size_eff == 1:
        # èˆŠè¡Œç‚ºï¼šå–®åŸ·è¡Œç·’ã€é€å¼µ
        session = requests.Session()
        for idx, file_path in enumerate(files, start=1):
            logging.info(f"({idx}/{len(files)}) ä¸Šå‚³ {file_path.name} ä¸­â€¦")
            result = uploadSingleImage(
                session=session,
                token=token,
                file_path=file_path,
                event_id=event_id,
                location=location,
                price=int(price_eff),
                bib_number=bib_number,
                timeout=float(timeout_eff),
                max_retries=int(max_retries_eff),
                retry_backoff=float(retry_backoff_eff),
                longitude=longitude_eff,
                latitude=latitude_eff,
            )
            results.append(result)
            # é€æª”å³æ™‚å¯«å…¥
            append_results([result], event_id=event_id, location=location)
    else:
        # ä½µç™¼ + æ‰¹æ¬¡ï¼ˆæ¯å€‹å·¥ä½œè™•ç†ä¸€å€‹æ‰¹æ¬¡ï¼‰
        batches = chunked(files, batch_size_eff)
        total_batches = len(batches)
        logging.info(f"å•Ÿç”¨ä½µç™¼èˆ‡/æˆ–æ‰¹æ¬¡ä¸Šå‚³ï¼šconcurrency={concurrency_eff}, batch_size={batch_size_eff}, æ‰¹æ¬¡æ•¸={total_batches}")

        def process_batch(idx_batch: int, batch_paths: List[Path]) -> List[UploadResult]:
            logging.info(f"[æ‰¹æ¬¡ {idx_batch}/{total_batches}] ä¸Šå‚³ {len(batch_paths)} å¼µâ€¦")
            return uploadImagesBatch(
                token=token,
                file_paths=batch_paths,
                event_id=event_id,
                location=location,
                price=int(price_eff),
                bib_number=bib_number,
                timeout=float(timeout_eff),
                max_retries=int(max_retries_eff),
                retry_backoff=float(retry_backoff_eff),
                longitude=longitude_eff,
                latitude=latitude_eff,
            )

        with ThreadPoolExecutor(max_workers=concurrency_eff) as executor:
            future_to_idx = {
                executor.submit(process_batch, i + 1, batch): i for i, batch in enumerate(batches)
            }
            for fut in as_completed(future_to_idx):
                try:
                    batch_results = fut.result()
                    results.extend(batch_results)
                    # é€æ‰¹å³æ™‚å¯«å…¥
                    append_results(batch_results, event_id=event_id, location=location)
                except Exception as e:
                    logging.exception(f"æ‰¹æ¬¡è™•ç†ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼š{e}")

    total = len(results)
    ok = sum(1 for r in results if r.success)
    fail = total - ok
    logging.info(f"å®Œæˆï¼šæˆåŠŸ {ok}ã€å¤±æ•— {fail}ã€ç¸½è¨ˆ {total}")


if __name__ == "__main__":
    main()
